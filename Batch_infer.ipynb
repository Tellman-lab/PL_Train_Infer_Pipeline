{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# This is MAC branch\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from einops import rearrange\n",
    "from scipy.special import softmax\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from PL_Support_Codes.models import build_model\n",
    "from PL_Support_Codes.tools import load_cfg_file\n",
    "from PL_Support_Codes.datasets.utils import generate_image_slice_object\n",
    "from PL_Support_Codes.utils.utils_image import ImageStitcher_v2 as ImageStitcher\n",
    "from PL_Support_Codes.datasets import build_dataset, tensors_and_lists_collate_fn\n",
    "\n",
    "from PL_Support_Codes.models.lf_model import LateFusionModel\n",
    "from PL_Support_Codes.models.ef_model import EarlyFusionModel\n",
    "from PL_Support_Codes.models.water_seg_model import WaterSegmentationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Setup model parameters\n",
    "dataset_name = \"batch_infer\"\n",
    "infer_split = \"all\"\n",
    "infer_seed_num = 0\n",
    "infer_train_split_pct = 0.0\n",
    "infer_num_workers = 0\n",
    "n_classes_model = 3\n",
    "model_used_here = \"unet_orig\" #unet_cbam\n",
    "# optimizer_used = \"adam\"\n",
    "model_loss_fn_a_infer = \"cross_entropy\"\n",
    "model_loss_fn_b_infer = \"cross_entropy\"\n",
    "model_loss_fn_a_infer_ratio = 1\n",
    "model_loss_fn_b_infer_ratio = 0\n",
    "\n",
    "\n",
    "base_save_dir = r\"E:\\Zhijie_PL_Pipeline\\Infered_result\\RGV30_CSDA_UNET_HPC\"\n",
    "checkpoint_path = r\"E:\\Zhijie_PL_Pipeline\\Trained_model\\UNET\\checkpoints\\CSDA_UNET_HPC.ckpt\"\n",
    "\n",
    "# Root folder containing the directories you waant to run inference on, under this folder, there should be different dates folder, within the dates folder, there should be imgs\n",
    "ROOT_FOLDER = r\"E:\\Zhijie_PL_Pipeline\\DATA\\RGV_local\\\\\"\n",
    "# JSON file path\n",
    "JSON_FILE = r\"E:\\Zhijie_PL_Pipeline\\Zhijie_PL_Pipeline\\dataset_dirs.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def infer():\n",
    "    # Load configuration file.\n",
    "    path_components = checkpoint_path.split(os.sep)\n",
    "    experiment_dir = os.sep.join(path_components[:4]) \n",
    "\n",
    "    cfg_path = os.path.join(experiment_dir, 'config.yaml')\n",
    "    print(\"check point file path: \", checkpoint_path)\n",
    "    cfg = load_cfg_file(cfg_path)\n",
    "\n",
    "    if 'model_n_classes' in cfg:\n",
    "        n_classes_used = cfg.model_n_classes\n",
    "    else:\n",
    "        n_classes_used = n_classes_model\n",
    "    \n",
    "    if 'model_used' in cfg:\n",
    "        model_used_infer = cfg.model_used\n",
    "    else:\n",
    "        model_used_infer = model_used_here\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    if not os.path.exists(base_save_dir):\n",
    "        os.makedirs(base_save_dir)\n",
    "    print(\"Saving inference to: \",base_save_dir)\n",
    "    # Load dataset.\n",
    "    slice_params = generate_image_slice_object(cfg.crop_height, cfg.crop_width, min(cfg.crop_height, cfg.crop_width))\n",
    "    eval_dataset = build_dataset(dataset_name,\n",
    "                                 infer_split,\n",
    "                                 slice_params,\n",
    "                                 sensor=cfg.dataset.sensor,\n",
    "                                 channels=cfg.dataset.channels,\n",
    "                                 n_classes=n_classes_used,\n",
    "                                 norm_mode=cfg.norm_mode,\n",
    "                                 eval_region=cfg.eval_region,\n",
    "                                 ignore_index=cfg.ignore_index,\n",
    "                                 seed_num=infer_seed_num,\n",
    "                                 train_split_pct=infer_train_split_pct,\n",
    "                                 output_metadata=True,\n",
    "                                 # ** allows us to pass in any additional arguments to the dataset as dictionary.\n",
    "                                 **cfg.dataset.dataset_kwargs)\n",
    "\n",
    "    eval_loader = DataLoader(eval_dataset,\n",
    "                             batch_size=cfg.batch_size,\n",
    "                             shuffle=False,\n",
    "                             num_workers=infer_num_workers, collate_fn=tensors_and_lists_collate_fn)\n",
    "    \n",
    "    MODELS = {\n",
    "        'ms_model': WaterSegmentationModel,\n",
    "        'ef_model': EarlyFusionModel,\n",
    "        'lf_model': LateFusionModel\n",
    "    }\n",
    "# here need to retrain model and save new config file, in the new one it\n",
    "#should be   cfg.model_used\n",
    "    model = MODELS[cfg.model.name].load_from_checkpoint(checkpoint_path,\n",
    "                                       in_channels=eval_dataset.n_channels,\n",
    "                                       n_classes=eval_dataset.n_classes,\n",
    "                                       lr=cfg.lr,\n",
    "                                       log_image_iter=cfg.log_image_iter,\n",
    "                                       to_rgb_fcn=eval_dataset.to_RGB,\n",
    "                                       ignore_index=eval_dataset.ignore_index,\n",
    "                                       model_used=model_used_infer,\n",
    "                                       model_loss_fn_a = model_loss_fn_a_infer,\n",
    "                                       model_loss_fn_b = model_loss_fn_b_infer,\n",
    "                                       model_loss_fn_a_ratio = model_loss_fn_a_infer_ratio,\n",
    "                                       model_loss_fn_b_ratio = model_loss_fn_b_infer_ratio,\n",
    "                                       **cfg.model.model_kwargs)\n",
    "    model._set_model_to_eval()\n",
    "\n",
    "    # Get device.\n",
    "    if torch.cuda.is_available():\n",
    "        device = 'cuda'\n",
    "        print(\"!!!!!! CUDA is available!!!!!!\")\n",
    "    else:\n",
    "        device = 'mps'\n",
    "        print(\"!!!!!! CUDA is not available, using MPS !!!!!!\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Generate predictions on target dataset.\n",
    "    pred_canvases = {}\n",
    "    with torch.no_grad(): #no_grad() prevents gradiant calculation, which is not needed for inference.\n",
    "        # breakpoint()\n",
    "        for batch in tqdm(eval_loader, colour='green', desc='Generating predictions'):\n",
    "            # Move batch to device.\n",
    "            for key, value in batch.items():\n",
    "                if isinstance(value, torch.Tensor):\n",
    "                    batch[key] = value.to(dtype=torch.float32).to(device)\n",
    "\n",
    "            # Generate predictions.\n",
    "            # this pass the current batch into model, to generate prediction, \n",
    "            #at this stage, the output is the raw output that is the probability distribution over the classes for the corresponding pixel in the input image. This distribution can be interpreted as \n",
    "            #the model's confidence in each class for that pixel. They are the raw score of what model thinks the possibility of each class for each pixel.\n",
    "            output = model(batch).detach().cpu().numpy()\n",
    "            # convert the each class probability distribution to the softmax probability distribution. meaning that the probabilities will add up to 1 between different classes\n",
    "            preds = softmax(output, axis=1)\n",
    "\n",
    "            input_images = batch['image'].detach().cpu().numpy()\n",
    "            # rearrange the tensor to the format of (batch, height, width, channel)\n",
    "            preds = rearrange(preds, 'b c h w -> b h w c')\n",
    "            input_images = rearrange(input_images, 'b c h w -> b h w c')\n",
    "            batch_mean = rearrange(batch['mean'], 'b c 1 1 -> b 1 1 c').detach().cpu().numpy()\n",
    "            batch_std = rearrange(batch['std'], 'b c 1 1 -> b 1 1 c').detach().cpu().numpy()\n",
    "\n",
    "            for b in range(output.shape[0]):# output.shape[0] is the batch size. so this code is iterating through each image in the batch\n",
    "\n",
    "                pred = preds[b]\n",
    "                metadata = batch['metadata'][b]\n",
    "                input_image = input_images[b]\n",
    "                region_name = metadata['region_name']\n",
    "\n",
    "                # Check if image stitcher exists for this region.\n",
    "                if region_name not in pred_canvases.keys():\n",
    "                    # Get base save directories.\n",
    "                    pred_save_dir = os.path.join(base_save_dir, region_name + '_pred')\n",
    "\n",
    "                    # Initialize image stitchers.\n",
    "                    pred_canvases[region_name] = ImageStitcher(pred_save_dir, save_backend='tifffile', save_ext='.tif')\n",
    "                \n",
    "                # Add input image and prediction to stitchers.\n",
    "                unnorm_img = (input_image * batch_std[b]) + batch_mean[b]\n",
    "                image_name = os.path.splitext(os.path.split(metadata['image_path'])[1])[0]\n",
    "                pred_canvases[region_name].add_image(pred, image_name, metadata['crop_params'], metadata['crop_params'].og_height, metadata['crop_params'].og_width)\n",
    "\n",
    "    # Convert stitched images to proper format.\n",
    "    for region_name in pred_canvases.keys():\n",
    "        # Combine images.\n",
    "        pred_canvas = pred_canvases[region_name].get_combined_images()\n",
    "\n",
    "        for image_name, image in pred_canvas.items():\n",
    "            # Figure out the predicted class.\n",
    "            pred = np.clip(image.argmax(axis=2), 0, 1)\n",
    "            # save_path = os.path.join(pred_canvases[region_name].save_dir, image_name + '.tif')\n",
    "            if not os.path.exists(os.path.join(base_save_dir, region_name)):\n",
    "                os.makedirs(os.path.join(base_save_dir, region_name))\n",
    "            save_path = os.path.join(base_save_dir, region_name, image_name + '.tif')\n",
    "            print(f'Saving {save_path}')\n",
    "            Image.fromarray((pred*255).astype('uint8')).save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the  1 th iteration, out of  1\n",
      "This is the  1 th iteration, out of  1\n",
      "We are infering  E:\\Zhijie_PL_Pipeline\\DATA\\RGV_local\\\\RGV_240603_30\n",
      "We are infering  E:\\Zhijie_PL_Pipeline\\DATA\\RGV_local\\\\RGV_240603_30\n",
      "check point file path:  E:\\Zhijie_PL_Pipeline\\Trained_model\\UNET\\checkpoints\\CSDA_UNET_HPC.ckpt\n",
      "Saving inference to:  E:\\Zhijie_PL_Pipeline\\Infered_result\\RGV30_CSDA_UNET_HPC\n",
      "RGV_240603_30\n",
      "Number of images in all dataset: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.8.2 to v2.2.5. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint E:\\Zhijie_PL_Pipeline\\Trained_model\\UNET\\checkpoints\\CSDA_UNET_HPC.ckpt`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model used!!!!!!!!!:  <class 'PL_Support_Codes.models.unet.UNet_Orig'>\n",
      "!!!!!!!!!!!!\n",
      "!!!!!!!!!!!!\n",
      "Model used:  unet_orig\n",
      "n_classes:  3\n",
      "in_channels:  {'ms_image': 4}\n",
      "ignore_index:  2\n",
      "optimizer_name:  adam\n",
      "0.0005\n",
      "!!!!!!!!!!!!\n",
      "!!!!!!!!!!!!\n",
      "!!!!!! CUDA is available!!!!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating predictions: 100%|\u001b[32m██████████\u001b[0m| 30/30 [00:13<00:00,  2.15it/s]\n",
      "Combining images: 100%|\u001b[32m██████████\u001b[0m| 30/30 [00:01<00:00, 21.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving E:\\Zhijie_PL_Pipeline\\Infered_result\\RGV30_CSDA_UNET_HPC\\RGV_240603_30\\20180623La_Paloma_Northeast.tif\n",
      "Saving E:\\Zhijie_PL_Pipeline\\Infered_result\\RGV30_CSDA_UNET_HPC\\RGV_240603_30\\20180623La_Paloma_SouthCentral.tif\n",
      "Saving E:\\Zhijie_PL_Pipeline\\Infered_result\\RGV30_CSDA_UNET_HPC\\RGV_240603_30\\20180623_Heidelberg_Indian_Hills_Capisallo_East.tif\n",
      "Saving E:\\Zhijie_PL_Pipeline\\Infered_result\\RGV30_CSDA_UNET_HPC\\RGV_240603_30\\20180623_Heidelberg_Indian_Hills_Capisallo_Far_North_West.tif\n",
      "Saving E:\\Zhijie_PL_Pipeline\\Infered_result\\RGV30_CSDA_UNET_HPC\\RGV_240603_30\\20180623_Heidelberg_Indian_Hills_Capisallo_North_West.tif\n",
      "Saving E:\\Zhijie_PL_Pipeline\\Infered_result\\RGV30_CSDA_UNET_HPC\\RGV_240603_30\\20180623_Heidelberg_Indian_Hills_Capisallo_Northeast.tif\n",
      "Saving E:\\Zhijie_PL_Pipeline\\Infered_result\\RGV30_CSDA_UNET_HPC\\RGV_240603_30\\20180623_Heidelberg_Indian_Hills_Capisallo_West.tif\n",
      "Saving E:\\Zhijie_PL_Pipeline\\Infered_result\\RGV30_CSDA_UNET_HPC\\RGV_240603_30\\20180623_Heidelberg_Indian_Hills_Capisallo_far_west.tif\n",
      "Saving E:\\Zhijie_PL_Pipeline\\Infered_result\\RGV30_CSDA_UNET_HPC\\RGV_240603_30\\20180623_La_Feria_East.tif\n",
      "Saving E:\\Zhijie_PL_Pipeline\\Infered_result\\RGV30_CSDA_UNET_HPC\\RGV_240603_30\\20180623_La_Feria_Northeast.tif\n",
      "Saving E:\\Zhijie_PL_Pipeline\\Infered_result\\RGV30_CSDA_UNET_HPC\\RGV_240603_30\\20180623_La_Feria_South_Central.tif\n",
      "Saving E:\\Zhijie_PL_Pipeline\\Infered_result\\RGV30_CSDA_UNET_HPC\\RGV_240603_30\\20180623_La_Feria_Southwest.tif\n",
      "Saving E:\\Zhijie_PL_Pipeline\\Infered_result\\RGV30_CSDA_UNET_HPC\\RGV_240603_30\\20180623_La_Paloma_North_Central.tif\n",
      "Saving E:\\Zhijie_PL_Pipeline\\Infered_result\\RGV30_CSDA_UNET_HPC\\RGV_240603_30\\20180623_La_Paloma_North_West.tif\n",
      "Saving E:\\Zhijie_PL_Pipeline\\Infered_result\\RGV30_CSDA_UNET_HPC\\RGV_240603_30\\20180623_La_Paloma_Southeast.tif\n",
      "Saving E:\\Zhijie_PL_Pipeline\\Infered_result\\RGV30_CSDA_UNET_HPC\\RGV_240603_30\\20180623_La_Paloma_Southwest.tif\n",
      "Saving E:\\Zhijie_PL_Pipeline\\Infered_result\\RGV30_CSDA_UNET_HPC\\RGV_240603_30\\20180623_Las_Milpas_North_Central.tif\n",
      "Saving E:\\Zhijie_PL_Pipeline\\Infered_result\\RGV30_CSDA_UNET_HPC\\RGV_240603_30\\20180623_Las_Milpas_South_Central.tif\n",
      "Saving E:\\Zhijie_PL_Pipeline\\Infered_result\\RGV30_CSDA_UNET_HPC\\RGV_240603_30\\20180623_Las_Milpas_Southeast.tif\n",
      "Saving E:\\Zhijie_PL_Pipeline\\Infered_result\\RGV30_CSDA_UNET_HPC\\RGV_240603_30\\20180623_Las_Milpas_Southwest.tif\n",
      "Saving E:\\Zhijie_PL_Pipeline\\Infered_result\\RGV30_CSDA_UNET_HPC\\RGV_240603_30\\20180623_Sahara_Estates_Alberta_Acres_West.tif\n",
      "Saving E:\\Zhijie_PL_Pipeline\\Infered_result\\RGV30_CSDA_UNET_HPC\\RGV_240603_30\\20180623_Spring_Glen_Northeast.tif\n",
      "Saving E:\\Zhijie_PL_Pipeline\\Infered_result\\RGV30_CSDA_UNET_HPC\\RGV_240603_30\\20180623_Spring_Glen_northwest.tif\n",
      "Saving E:\\Zhijie_PL_Pipeline\\Infered_result\\RGV30_CSDA_UNET_HPC\\RGV_240603_30\\20180623_Spring_Glen_southeast.tif\n",
      "Saving E:\\Zhijie_PL_Pipeline\\Infered_result\\RGV30_CSDA_UNET_HPC\\RGV_240603_30\\20180623_Spring_Glen_southwest.tif\n",
      "Saving E:\\Zhijie_PL_Pipeline\\Infered_result\\RGV30_CSDA_UNET_HPC\\RGV_240603_30\\20190626_Esquina_Northeast.tif\n",
      "Saving E:\\Zhijie_PL_Pipeline\\Infered_result\\RGV30_CSDA_UNET_HPC\\RGV_240603_30\\20190626_Hargill_Edcouch_Elsa_East_Central.tif\n",
      "Saving E:\\Zhijie_PL_Pipeline\\Infered_result\\RGV30_CSDA_UNET_HPC\\RGV_240603_30\\20190626_Hargill_Edcouch_Elsa_Far_North_East.tif\n",
      "Saving E:\\Zhijie_PL_Pipeline\\Infered_result\\RGV30_CSDA_UNET_HPC\\RGV_240603_30\\20190626_La_Pampa_South_east.tif\n",
      "Saving E:\\Zhijie_PL_Pipeline\\Infered_result\\RGV30_CSDA_UNET_HPC\\RGV_240603_30\\20190626_La_Pampa_east.tif\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "counter = 1\n",
    "# Loop through each sub-directory in the root folder\n",
    "for dir in os.listdir(ROOT_FOLDER):\n",
    "    full_dir_path = os.path.join(ROOT_FOLDER, dir)\n",
    "    if os.path.isdir(full_dir_path):\n",
    "        FOLDER_NAME = full_dir_path\n",
    "\n",
    "        # Update the JSON file\n",
    "        with open(JSON_FILE, 'r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "            data['batch_infer'] = FOLDER_NAME\n",
    "\n",
    "        with open(JSON_FILE, 'w') as json_file:\n",
    "            json.dump(data, json_file)\n",
    "\n",
    "        # Execute the command\n",
    "        print(\"This is the \", counter, \"th iteration, out of \", len(os.listdir(ROOT_FOLDER)))\n",
    "        print(\"This is the \", counter, \"th iteration, out of \", len(os.listdir(ROOT_FOLDER)))\n",
    "        print(\"We are infering \", full_dir_path)\n",
    "        print(\"We are infering \", full_dir_path)\n",
    "        infer()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_path = r\"E:\\Zhijie_PL_Pipeline\\Trained_model\\Unet_PS_models\\checkpoints\\model-epoch=06-val_MulticlassJaccardIndex=0.8755.ckpt\"\n",
    "# path_components = checkpoint_path.split(os.sep)\n",
    "# experiment_dir = os.sep.join(path_components[:4]) \n",
    "# # experiment_dir = os.path.dirname(checkpoint_path)\n",
    "# # experiment_dir = os.path.join(*checkpoint_path.split(os.sep)[:-2])\n",
    "\n",
    "# print(\"experiment_dir: \", experiment_dir)\n",
    "# print(checkpoint_path.split('\\\\')[:-2])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geotorchee",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

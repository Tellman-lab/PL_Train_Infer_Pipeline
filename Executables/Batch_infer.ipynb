{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from einops import rearrange\n",
    "from scipy.special import softmax\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from PL_Support_Codes.models import build_model\n",
    "from PL_Support_Codes.tools import load_cfg_file\n",
    "from PL_Support_Codes.datasets.utils import generate_image_slice_object\n",
    "from PL_Support_Codes.utils.utils_image import ImageStitcher_v2 as ImageStitcher\n",
    "from PL_Support_Codes.datasets import build_dataset, tensors_and_lists_collate_fn\n",
    "\n",
    "from PL_Support_Codes.models.lf_model import LateFusionModel\n",
    "from PL_Support_Codes.models.ef_model import EarlyFusionModel\n",
    "from PL_Support_Codes.models.water_seg_model import WaterSegmentationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer():\n",
    "    # parser = argparse.ArgumentParser()\n",
    "    # # parser.add_argument('checkpoint_path',\n",
    "    # #                     type=str,\n",
    "    # #                     help='Path to the checkpoint file')\n",
    "    # # parser.add_argument('dataset_name', type=str, help='Name of the dataset')\n",
    "    # # parser.add_argument('split', type=str, help='Split of the dataset')\n",
    "    # # parser.add_argument('--n_workers',\n",
    "    # #                     type=int,\n",
    "    # #                     default=None,\n",
    "    # #                     help='Number of workers for the data loader')\n",
    "    \n",
    "    # args = parser.parse_args()\n",
    "    #TODO:\n",
    "    # Create save directory.\n",
    "    base_save_dir = \"E:\\\\Zhijie_PL_Pipeline\\\\Infered_result\\\\trif1\"\n",
    "    checkpoint_path = \"E:\\\\Zhijie_PL_Pipeline\\\\Trained_model\\\\CBAM\\\\checkpoints\\\\THP_CBAM_HPC.ckpt\"\n",
    "    dataset_name = \"thp_timeseries\"\n",
    "    infer_split = \"all\"\n",
    "    #TODO:\n",
    "\n",
    "    # Load configuration file.\n",
    "    experiment_dir = '\\\\'.join(checkpoint_path.split('\\\\')[:-2])\n",
    "    cfg_path = os.path.join(experiment_dir, 'config.yaml')\n",
    "    print(\"check point file path: \", checkpoint_path)\n",
    "    cfg = load_cfg_file(cfg_path)\n",
    "\n",
    "    # ## Update config parameters.\n",
    "    # if args.n_workers is None:\n",
    "    #     cfg.n_workers = cfg.n_workers\n",
    "    # else:\n",
    "    #     cfg.n_workers = args.n_workers\n",
    "\n",
    "    # if hasattr(cfg, 'seed_num') is False:\n",
    "    #     cfg.seed_num = None\n",
    "\n",
    "    # if hasattr(cfg, 'train_split_pct') is False:\n",
    "    #     cfg.train_split_pct = 0.0\n",
    "\n",
    "\n",
    "    if not os.path.exists(base_save_dir):\n",
    "        os.makedirs(base_save_dir)\n",
    "    print(\"Saving inference to: \",base_save_dir)\n",
    "    # Load dataset.\n",
    "    slice_params = generate_image_slice_object(cfg.crop_height, cfg.crop_width, min(cfg.crop_height, cfg.crop_width))\n",
    "    eval_dataset = build_dataset(dataset_name,\n",
    "                                 infer_split,\n",
    "                                 slice_params,\n",
    "                                 sensor=cfg.dataset.sensor,\n",
    "                                 channels=cfg.dataset.channels,\n",
    "                                 norm_mode=cfg.norm_mode,\n",
    "                                 eval_region=cfg.eval_region,\n",
    "                                 ignore_index=cfg.ignore_index,\n",
    "                                 seed_num=cfg.seed_num,\n",
    "                                 train_split_pct=cfg.train_split_pct,\n",
    "                                 output_metadata=True,\n",
    "                                 # ** allows us to pass in any additional arguments to the dataset as dictionary.\n",
    "                                 **cfg.dataset.dataset_kwargs)\n",
    "\n",
    "    eval_loader = DataLoader(eval_dataset,\n",
    "                             batch_size=cfg.batch_size,\n",
    "                             shuffle=False,\n",
    "                             num_workers=cfg.n_workers, collate_fn=tensors_and_lists_collate_fn)\n",
    "    \n",
    "    MODELS = {\n",
    "        'ms_model': WaterSegmentationModel,\n",
    "        'ef_model': EarlyFusionModel,\n",
    "        'lf_model': LateFusionModel\n",
    "    }\n",
    "    \n",
    "    model = MODELS[cfg.model.name].load_from_checkpoint(checkpoint_path,\n",
    "                                       in_channels=eval_dataset.n_channels,\n",
    "                                       n_classes=eval_dataset.n_classes,\n",
    "                                       lr=cfg.lr,\n",
    "                                       log_image_iter=cfg.log_image_iter,\n",
    "                                       to_rgb_fcn=eval_dataset.to_RGB,\n",
    "                                       ignore_index=eval_dataset.ignore_index,\n",
    "                                       **cfg.model.model_kwargs)\n",
    "    model._set_model_to_eval()\n",
    "\n",
    "    # Get device.\n",
    "    if torch.cuda.is_available():\n",
    "        device = 'cuda'\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Generate predictions on target dataset.\n",
    "    pred_canvases = {}\n",
    "    with torch.no_grad():\n",
    "        # breakpoint()\n",
    "        for batch in tqdm(eval_loader, colour='green', desc='Generating predictions'):\n",
    "            # Move batch to device.\n",
    "            for key, value in batch.items():\n",
    "                if isinstance(value, torch.Tensor):\n",
    "                    batch[key] = value.to(device)\n",
    "\n",
    "            # Generate predictions.\n",
    "            output = model(batch).detach().cpu().numpy()\n",
    "            preds = softmax(output, axis=1)\n",
    "\n",
    "            input_images = batch['image'].detach().cpu().numpy()\n",
    "\n",
    "            preds = rearrange(preds, 'b c h w -> b h w c')\n",
    "            input_images = rearrange(input_images, 'b c h w -> b h w c')\n",
    "            batch_mean = rearrange(batch['mean'], 'b c 1 1 -> b 1 1 c').detach().cpu().numpy()\n",
    "            batch_std = rearrange(batch['std'], 'b c 1 1 -> b 1 1 c').detach().cpu().numpy()\n",
    "\n",
    "            for b in range(output.shape[0]):\n",
    "\n",
    "                pred = preds[b]\n",
    "                metadata = batch['metadata'][b]\n",
    "                input_image = input_images[b]\n",
    "                region_name = metadata['region_name']\n",
    "\n",
    "                # Check if image stitcher exists for this region.\n",
    "                if region_name not in pred_canvases.keys():\n",
    "                    # Get base save directories.\n",
    "                    pred_save_dir = os.path.join(base_save_dir, region_name + '_pred')\n",
    "\n",
    "                    # Initialize image stitchers.\n",
    "                    pred_canvases[region_name] = ImageStitcher(pred_save_dir, save_backend='tifffile', save_ext='.tif')\n",
    "                \n",
    "                # Add input image and prediction to stitchers.\n",
    "                unnorm_img = (input_image * batch_std[b]) + batch_mean[b]\n",
    "                image_name = os.path.splitext(os.path.split(metadata['image_path'])[1])[0]\n",
    "                pred_canvases[region_name].add_image(pred, image_name, metadata['crop_params'], metadata['crop_params'].og_height, metadata['crop_params'].og_width)\n",
    "\n",
    "    # Convert stitched images to proper format.\n",
    "    for region_name in pred_canvases.keys():\n",
    "        # Combine images.\n",
    "        pred_canvas = pred_canvases[region_name].get_combined_images()\n",
    "\n",
    "        for image_name, image in pred_canvas.items():\n",
    "            # Figure out the predicted class.\n",
    "            pred = np.clip(image.argmax(axis=2), 0, 1)\n",
    "            save_path = os.path.join(pred_canvases[region_name].save_dir, image_name + '.tif')\n",
    "            print(f'Saving {save_path}')\n",
    "            Image.fromarray((pred*255).astype('uint8')).save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check point file path:  E:\\Zhijie_PL_Pipeline\\Trained_model\\CBAM\\checkpoints\\THP_CBAM_HPC.ckpt\n",
      "Saving inference to:  E:\\Zhijie_PL_Pipeline\\Infered_result\\trif1\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'e:\\\\Zhijie_PL_Pipeline\\\\Zhijie_PL_Pipeline\\\\Executables\\\\dataset_dirs.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(data, json_file)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Execute the command\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m \u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[20], line 46\u001b[0m, in \u001b[0;36minfer\u001b[1;34m()\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Load dataset.\u001b[39;00m\n\u001b[0;32m     45\u001b[0m slice_params \u001b[38;5;241m=\u001b[39m generate_image_slice_object(cfg\u001b[38;5;241m.\u001b[39mcrop_height, cfg\u001b[38;5;241m.\u001b[39mcrop_width, \u001b[38;5;28mmin\u001b[39m(cfg\u001b[38;5;241m.\u001b[39mcrop_height, cfg\u001b[38;5;241m.\u001b[39mcrop_width))\n\u001b[1;32m---> 46\u001b[0m eval_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m                             \u001b[49m\u001b[43minfer_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mslice_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m                             \u001b[49m\u001b[43msensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mchannels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchannels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mnorm_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m                             \u001b[49m\u001b[43meval_region\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_region\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mseed_num\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseed_num\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mtrain_split_pct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_split_pct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m                             \u001b[49m\u001b[43moutput_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;66;43;03m# ** allows us to pass in any additional arguments to the dataset as dictionary.\u001b[39;49;00m\n\u001b[0;32m     58\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m eval_loader \u001b[38;5;241m=\u001b[39m DataLoader(eval_dataset,\n\u001b[0;32m     61\u001b[0m                          batch_size\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mbatch_size,\n\u001b[0;32m     62\u001b[0m                          shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     63\u001b[0m                          num_workers\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mn_workers, collate_fn\u001b[38;5;241m=\u001b[39mtensors_and_lists_collate_fn)\n\u001b[0;32m     65\u001b[0m MODELS \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mms_model\u001b[39m\u001b[38;5;124m'\u001b[39m: WaterSegmentationModel,\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mef_model\u001b[39m\u001b[38;5;124m'\u001b[39m: EarlyFusionModel,\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlf_model\u001b[39m\u001b[38;5;124m'\u001b[39m: LateFusionModel\n\u001b[0;32m     69\u001b[0m }\n",
      "File \u001b[1;32mE:\\Zhijie_PL_Pipeline\\Zhijie_PL_Pipeline\\PL_Support_Codes\\datasets\\__init__.py:43\u001b[0m, in \u001b[0;36mbuild_dataset\u001b[1;34m(dset_name, split, slice_params, eval_region, sensor, channels, **kwargs)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_dataset\u001b[39m(dset_name, split, slice_params, eval_region, sensor,\n\u001b[0;32m     42\u001b[0m                   channels, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 43\u001b[0m     dset_root_dir \u001b[38;5;241m=\u001b[39m \u001b[43mget_dset_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdset_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;66;03m# Only directly input required parameters.\u001b[39;00m\n\u001b[0;32m     47\u001b[0m         dataset \u001b[38;5;241m=\u001b[39m DATASETS[dset_name](dset_root_dir,\n\u001b[0;32m     48\u001b[0m                                       split,\n\u001b[0;32m     49\u001b[0m                                       slice_params,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     52\u001b[0m                                       sensor\u001b[38;5;241m=\u001b[39msensor,\n\u001b[0;32m     53\u001b[0m                                       \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\Zhijie_PL_Pipeline\\Zhijie_PL_Pipeline\\PL_Support_Codes\\datasets\\utils.py:17\u001b[0m, in \u001b[0;36mget_dset_path\u001b[1;34m(dset_name)\u001b[0m\n\u001b[0;32m     15\u001b[0m     base_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetcwd()\n\u001b[0;32m     16\u001b[0m root_dirs_file_paths \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(base_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset_dirs.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m root_dirs \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mroot_dirs_file_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     18\u001b[0m dset_root_dir \u001b[38;5;241m=\u001b[39m root_dirs[dset_name]\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dset_root_dir\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'e:\\\\Zhijie_PL_Pipeline\\\\Zhijie_PL_Pipeline\\\\Executables\\\\dataset_dirs.json'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "# import subprocess\n",
    "\n",
    "# Root folder containing the directories\n",
    "ROOT_FOLDER = \"E:\\\\Zhijie_PL_Pipeline\\\\DATA\\\\trif_example\\\\\"\n",
    "\n",
    "# JSON file path\n",
    "JSON_FILE = \"E:\\\\Zhijie_PL_Pipeline\\\\Zhijie_PL_Pipeline\\\\dataset_dirs.json\"\n",
    "\n",
    "# Loop through each sub-directory in the root folder\n",
    "for dir in os.listdir(ROOT_FOLDER):\n",
    "    full_dir_path = os.path.join(ROOT_FOLDER, dir)\n",
    "    if os.path.isdir(full_dir_path):\n",
    "        # Extract the folder name\n",
    "        FOLDER_NAME = os.path.basename(full_dir_path)\n",
    "\n",
    "        # Update the JSON file\n",
    "        with open(JSON_FILE, 'r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "            data['thp_timeseries'] = FOLDER_NAME\n",
    "\n",
    "        with open(JSON_FILE, 'w') as json_file:\n",
    "            json.dump(data, json_file)\n",
    "\n",
    "        # Execute the command\n",
    "        infer()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geotorchee",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
